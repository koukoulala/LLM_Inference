nohup: ignoring input
Number of GPUs specified by CUDA_VISIBLE_DEVICES: 1
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:02,  2.93it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:02,  2.99it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:01,  3.03it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:01<00:01,  2.46it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:01<00:01,  2.65it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:02<00:00,  2.75it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:02<00:00,  2.81it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  3.29it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  2.95it/s]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Processing 0th text
Processing 100th text
Total number of texts: 200
Average time to complete texts:  2.557
Average tokens/sec:  0.854
Inference completed
